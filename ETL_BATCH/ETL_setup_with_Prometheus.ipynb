{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup notebook for BATCH processing and ETL, which fills the analytics DB every day at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements needed to be in docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "psycopg2-binary\n",
    "python-dotenv\n",
    "requests\n",
    "pymongo\n",
    "clickhouse-connect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env file for ETL python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./.env\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./.env\n",
    "MONGO_MANAGER_DATABASE_R_ONLY_URI=mongodb://external:external_pass@mongo_manager_db_service:27017/ParkMan_manager_db\n",
    "MONGO_USER_DATABASE_R_ONLY_URI=mongodb://external:external_pass@mongo_user_db_service:27017/ParkMan_user_db\n",
    "TIMESCALE_DB_DATABASE_URI=postgresql://postgres_user:postgres_pass@park_transactions_db:5432/ParkingTransactionsDB\n",
    "CLICKHOUSE_PORT=8123\n",
    "CLICKHOUSE_HOST=clickhouse\n",
    "CLICKHOUSE_USER=parkman_user\n",
    "CLICKHOUSE_PASS=parkman_user_pass\n",
    "PYTHONUNBUFFERED=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dockerfile for ETL job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "# Use the official Python image from the Docker Hub\n",
    "FROM python:3.9-slim\n",
    "\n",
    "\n",
    "#install cron\n",
    "RUN apt-get update && apt-get install -y cron\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /app\n",
    "\n",
    "\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install the Python dependencies specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Copy the cron job file to the cron.d directory\n",
    "COPY cronjob /etc/cron.d/cronjob\n",
    "\n",
    "# Give execution rights on the cron job\n",
    "RUN chmod 0644 /etc/cron.d/cronjob\n",
    "RUN chmod a+x /app/etl_script.py\n",
    "\n",
    "# Apply the cron job\n",
    "RUN crontab /etc/cron.d/cronjob\n",
    "\n",
    "# Create the log file to be able to run tail\n",
    "RUN touch /var/log/cron.log\n",
    "\n",
    "\n",
    "\n",
    "# Run the Python script\n",
    "CMD [\"bash\",\"-c\",\"cron && tail -f /var/log/cron.log\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create cronjob which will execute python (setup good time - will run before midnight, but actually runs every 3 minutes for demonstration purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cronjob\n"
     ]
    }
   ],
   "source": [
    "%%writefile cronjob\n",
    "*/3 * * * * /usr/local/bin/python /app/etl_script.py 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write docker-compose for this service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../python-etl.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../python-etl.yml\n",
    "version: '3'\n",
    "\n",
    "services:\n",
    "  etl:\n",
    "    build: ./ETL_BATCH\n",
    "    container_name: etl_container\n",
    "    env_file:\n",
    "        - ./ETL_BATCH/.env\n",
    "    depends_on:\n",
    "        mongo_manager_db_service:\n",
    "            condition: service_healthy\n",
    "        mongo_user_db_service:\n",
    "            condition: service_healthy\n",
    "        clickhouse:\n",
    "            condition: service_healthy\n",
    "\n",
    "    networks:\n",
    "        - app-network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting etl_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etl_script.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from psycopg2 import connect \n",
    "from psycopg2.extras import RealDictCursor\n",
    "from datetime import datetime, timedelta\n",
    "from bson import ObjectId\n",
    "import clickhouse_connect\n",
    "\n",
    "\n",
    "### load env\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "#### MongoDB config\n",
    "\n",
    "MONGO_EXTERAL_USER_URI = os.getenv(\"MONGO_USER_DATABASE_R_ONLY_URI\", \"mongodb://external:external_pass@localhost:27017/ParkMan_user_db\")\n",
    "client_external_user_db = MongoClient(MONGO_EXTERAL_USER_URI)\n",
    "db_external_user = client_external_user_db.get_database()\n",
    "\n",
    "MONGO_EXTERNAL_MANAGER=os.getenv(\"MONGO_MANAGER_DATABASE_R_ONLY_URI\", \"mongodb://external:external_pass@localhost:27016/ParkMan_manager_db\")\n",
    "external_client_managerDB=MongoClient(MONGO_EXTERNAL_MANAGER)\n",
    "db_external_manager=external_client_managerDB.get_database()\n",
    "\n",
    "### Timescale DB connection\n",
    "TIMESCALE_DB_URI=os.getenv(\"TIMESCALE_DB_DATABASE_URI\", \"postgresql://postgres_user:postgres_pass@localhost:5432/ParkingTransactionsDB\")\n",
    "timescale_conn=connect(TIMESCALE_DB_URI)\n",
    "timescale_cursor=timescale_conn.cursor(cursor_factory=RealDictCursor)\n",
    "\n",
    "\n",
    "\n",
    "###clickhouse target connect\n",
    "CLICKHOUSE_PORT=os.getenv(\"CLICKHOUSE_PORT\", 8123)\n",
    "CLICKHOUSE_HOST=os.getenv(\"CLICKHOUSE_HOST\", \"localhost\")\n",
    "CLICKHOUSE_USER=os.getenv(\"CLICKHOUSE_USER\", \"parkman_user\")\n",
    "CLICKHOUSE_PASS=os.getenv(\"CLICKHOUSE_PASS\", \"parkman_user_pass\")\n",
    "\n",
    "client = clickhouse_connect.get_client(host=CLICKHOUSE_HOST, port=CLICKHOUSE_PORT, user=CLICKHOUSE_USER, password=CLICKHOUSE_PASS)\n",
    "\n",
    "\n",
    "##Time range\n",
    "\n",
    "\n",
    "end_range= datetime.now()\n",
    "\n",
    "begin_range = end_range - timedelta(hours=24)\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_timescale(begin_range, end_range):\n",
    "    ##get all data from timescale fitting range:\n",
    "    query_timescale = \"\"\"\n",
    "        SELECT * FROM parking_transactions\n",
    "        WHERE entry_timestamp >= %s AND ( exit_timestamp <= %s OR exit_timestamp IS NULL );\n",
    "    \"\"\"\n",
    "    timescale_cursor.execute(query_timescale, (begin_range, end_range))\n",
    "    data_timescale = timescale_cursor.fetchall()\n",
    "\n",
    "    filtered_ids = {\n",
    "            'parking_lot_ids': list(set(ObjectId(entry['parking_lot_id']) for entry in data_timescale)),\n",
    "            'user_ids': list(set(ObjectId(entry['user_id']) for entry in data_timescale))\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return data_timescale, filtered_ids\n",
    "\n",
    "def get_filtered_data_from_mongo(filtered_ids):\n",
    "    user_data=db_external_user.users\n",
    "\n",
    "    owner_data=db_external_manager.owners\n",
    "    parking_lot_data=db_external_manager.parking_lots\n",
    "    parking_space_data=db_external_manager.parking_spaces\n",
    "\n",
    "    parking_lot_ids = filtered_ids['parking_lot_ids']\n",
    "    user_ids = filtered_ids['user_ids']\n",
    "\n",
    "    parking_lots = {parking_lot['_id']: (parking_lot['name'], len(parking_lot['parking_spaces'])) for parking_lot in parking_lot_data.find({'_id': {'$in': parking_lot_ids}})}\n",
    "    users = {user['_id']: user['name']+\" \" + user['surname'] for user in user_data.find({'_id':  {'$in': user_ids}}) }\n",
    "\n",
    "    owners = {owner['_id']: (owner['name'] + \" \" + owner['surname'], owner['parking_lots']) for owner in owner_data.find({'parking_lots': {'$in': parking_lot_ids}})}\n",
    "\n",
    "    return parking_lots, users, owners\n",
    "\n",
    "\n",
    "def find_appropriate_owner(owner,parking_lot_id):\n",
    "    \n",
    "    for (key,val) in owner.items():\n",
    "        if ObjectId(parking_lot_id) in val[1]:\n",
    "            return (key,val[0])\n",
    "\n",
    "def merge_data(data_timescale, parking_lots, users, owner):\n",
    "\n",
    "    expanded_data = []\n",
    "    for entry in data_timescale:\n",
    "        parking_lot_id=(entry['parking_lot_id'])\n",
    "        user_id=entry['user_id']\n",
    "        owner_id, full_name=find_appropriate_owner(owner, parking_lot_id)\n",
    "        expanded_entry={\n",
    "            'owner_id': owner_id,\n",
    "            'owner_full_name': full_name,\n",
    "            'parking_lot_id': parking_lot_id,\n",
    "            'parking_lot_name': parking_lots.get(ObjectId(parking_lot_id))[0],\n",
    "            'parking_spot_number': parking_lots.get(ObjectId(parking_lot_id))[1],\n",
    "            'user_id': user_id,\n",
    "            'user_full_name': users.get(ObjectId(user_id)),\n",
    "            'entry_timestamp': entry['entry_timestamp'],\n",
    "            'leaving_timestamp': entry['exit_timestamp'],\n",
    "            'checkout_price': entry['checkout_price']\n",
    "        }\n",
    "\n",
    "        expanded_data.append(expanded_entry)\n",
    "    return expanded_data\n",
    "\n",
    "    \n",
    "def import_data_into_click_house(data):\n",
    "    query = \"\"\"\n",
    "    INSERT INTO parking_db.parking_analytics (owner_id, owner_full_name, parking_lot_id, parking_lot_name, parking_spot_number, user_id, user_full_name, entry_timestamp, leaving_timestamp, checkout_price)\n",
    "    VALUES\n",
    "    \"\"\"\n",
    "    values = ', '.join(f\"('{entry['owner_id']}', '{entry['owner_full_name']}', '{entry['parking_lot_id']}', '{entry['parking_lot_name']}', '{entry['parking_spot_number']}', '{entry['user_id']}', '{entry['user_full_name']}', '{entry['entry_timestamp']}', '{entry['leaving_timestamp']}','{entry['checkout_price']}'  )\" for entry in data)\n",
    "    query += values\n",
    "    client.command(query)\n",
    "\n",
    "\n",
    "\n",
    "data_timescale, filtered_ids= get_data_from_timescale(begin_range, end_range)\n",
    "parking_lots,users,owner=get_filtered_data_from_mongo(filtered_ids)\n",
    "\n",
    "merged=merge_data(data_timescale,parking_lots,users,owner)\n",
    "\n",
    "\n",
    "import_data_into_click_house(merged)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Imported data into clickhouse range {begin_range}, {end_range}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying ETL so that it contains custom metrics, which are imported in Prometheus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to modify requirements so that they install appropriate modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "psycopg2-binary\n",
    "python-dotenv\n",
    "requests\n",
    "pymongo\n",
    "clickhouse-connect\n",
    "prometheus_client\n",
    "schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cronjob is no loger needed since python will use scheduling and run a server for Prometheus to log. Dockerfile must be edited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "# Use the official Python image from the Docker Hub\n",
    "FROM python:3.9-slim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /app\n",
    "\n",
    "\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install the Python dependencies specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "\n",
    "\n",
    "# Give execution rights on the cron job\n",
    "RUN chmod a+x /app/etl_script.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the Python script\n",
    "CMD [\"bash\",\"-c\",\"python /app/etl_script.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, must modify python script to incorporate Prometheus metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting etl_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etl_script.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from psycopg2 import connect \n",
    "from psycopg2.extras import RealDictCursor\n",
    "from datetime import datetime, timedelta\n",
    "from bson import ObjectId\n",
    "import clickhouse_connect\n",
    "from prometheus_client import *\n",
    "import schedule\n",
    "import time as tm \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Prometheus metrics\n",
    "\n",
    "ETL_EXECUTION_TIME = Summary('etl_execution_time', 'Time spent processing an ETL run')\n",
    "LAST_EXECUTION_TIME = Gauge('last_etl_execution', 'Timestamp of the last ETL execution')\n",
    "ETL_RECORDS_PROCESSED = Counter('etl_records_processed', 'Total number of records processed by the ETL')\n",
    "ETL_THROUGHPUT = Gauge('etl_throughput', 'Records processed per second')\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_timescale(begin_range, end_range):\n",
    "    ##get all data from timescale fitting range:\n",
    "    query_timescale = \"\"\"\n",
    "        SELECT * FROM parking_transactions\n",
    "        WHERE entry_timestamp >= %s AND ( exit_timestamp <= %s OR exit_timestamp IS NULL );\n",
    "    \"\"\"\n",
    "    timescale_cursor.execute(query_timescale, (begin_range, end_range))\n",
    "    data_timescale = timescale_cursor.fetchall()\n",
    "\n",
    "    filtered_ids = {\n",
    "            'parking_lot_ids': list(set(ObjectId(entry['parking_lot_id']) for entry in data_timescale)),\n",
    "            'user_ids': list(set(ObjectId(entry['user_id']) for entry in data_timescale))\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return data_timescale, filtered_ids\n",
    "\n",
    "def get_filtered_data_from_mongo(filtered_ids):\n",
    "    user_data=db_external_user.users\n",
    "\n",
    "    owner_data=db_external_manager.owners\n",
    "    parking_lot_data=db_external_manager.parking_lots\n",
    "    parking_space_data=db_external_manager.parking_spaces\n",
    "\n",
    "    parking_lot_ids = filtered_ids['parking_lot_ids']\n",
    "    user_ids = filtered_ids['user_ids']\n",
    "\n",
    "    parking_lots = {parking_lot['_id']: (parking_lot['name'], len(parking_lot['parking_spaces'])) for parking_lot in parking_lot_data.find({'_id': {'$in': parking_lot_ids}})}\n",
    "    users = {user['_id']: user['name']+\" \" + user['surname'] for user in user_data.find({'_id':  {'$in': user_ids}}) }\n",
    "\n",
    "    owners = {owner['_id']: (owner['name'] + \" \" + owner['surname'], owner['parking_lots']) for owner in owner_data.find({'parking_lots': {'$in': parking_lot_ids}})}\n",
    "\n",
    "    return parking_lots, users, owners\n",
    "\n",
    "\n",
    "def find_appropriate_owner(owner,parking_lot_id):\n",
    "    \n",
    "    for (key,val) in owner.items():\n",
    "        if ObjectId(parking_lot_id) in val[1]:\n",
    "            return (key,val[0])\n",
    "\n",
    "def merge_data(data_timescale, parking_lots, users, owner):\n",
    "\n",
    "    expanded_data = []\n",
    "    for entry in data_timescale:\n",
    "        parking_lot_id=(entry['parking_lot_id'])\n",
    "        user_id=entry['user_id']\n",
    "        owner_id, full_name=find_appropriate_owner(owner, parking_lot_id)\n",
    "        expanded_entry={\n",
    "            'owner_id': owner_id,\n",
    "            'owner_full_name': full_name,\n",
    "            'parking_lot_id': parking_lot_id,\n",
    "            'parking_lot_name': parking_lots.get(ObjectId(parking_lot_id))[0],\n",
    "            'parking_spot_number': parking_lots.get(ObjectId(parking_lot_id))[1],\n",
    "            'user_id': user_id,\n",
    "            'user_full_name': users.get(ObjectId(user_id)),\n",
    "            'entry_timestamp': entry['entry_timestamp'],\n",
    "            'leaving_timestamp': entry['exit_timestamp'],\n",
    "            'checkout_price': entry['checkout_price']\n",
    "        }\n",
    "\n",
    "        expanded_data.append(expanded_entry)\n",
    "    return expanded_data\n",
    "\n",
    "    \n",
    "def import_data_into_click_house(data):\n",
    "    query = \"\"\"\n",
    "    INSERT INTO parking_db.parking_analytics (owner_id, owner_full_name, parking_lot_id, parking_lot_name, parking_spot_number, user_id, user_full_name, entry_timestamp, leaving_timestamp, checkout_price)\n",
    "    VALUES\n",
    "    \"\"\"\n",
    "    values = ', '.join(f\"('{entry['owner_id']}', '{entry['owner_full_name']}', '{entry['parking_lot_id']}', '{entry['parking_lot_name']}', '{entry['parking_spot_number']}', '{entry['user_id']}', '{entry['user_full_name']}', '{entry['entry_timestamp']}', '{entry['leaving_timestamp']}','{entry['checkout_price']}'  )\" for entry in data)\n",
    "    query += values\n",
    "    client.command(query)\n",
    "\n",
    "\n",
    "\n",
    "def exec_etl():\n",
    "\n",
    "    start=tm.time()\n",
    "    \n",
    "    ## Get time range\n",
    "    end_range= datetime.now()\n",
    "\n",
    "    begin_range = end_range - timedelta(hours=24)\n",
    "    ##\n",
    "\n",
    "    ##Do ETL\n",
    "    data_timescale, filtered_ids= get_data_from_timescale(begin_range, end_range)\n",
    "    parking_lots,users,owner=get_filtered_data_from_mongo(filtered_ids)\n",
    "\n",
    "    merged=merge_data(data_timescale,parking_lots,users,owner)\n",
    "    \n",
    "\n",
    "    import_data_into_click_house(merged)\n",
    "    ##\n",
    "\n",
    "    end=tm.time()\n",
    "\n",
    "\n",
    "    #Measure how long it executed\n",
    "    ETL_EXECUTION_TIME.observe(end-start)\n",
    "\n",
    "    # Update throughput\n",
    "    ETL_THROUGHPUT.set(len(merged) / (end-start))\n",
    "\n",
    "    #increment counter\n",
    "    ETL_RECORDS_PROCESSED.inc(len(merged))\n",
    "\n",
    "    LAST_EXECUTION_TIME.set_to_current_time()\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    ### load env\n",
    "    load_dotenv()\n",
    "\n",
    "\n",
    "    #### MongoDB config\n",
    "\n",
    "    MONGO_EXTERAL_USER_URI = os.getenv(\"MONGO_USER_DATABASE_R_ONLY_URI\", \"mongodb://external:external_pass@localhost:27017/ParkMan_user_db\")\n",
    "    client_external_user_db = MongoClient(MONGO_EXTERAL_USER_URI)\n",
    "    db_external_user = client_external_user_db.get_database()\n",
    "\n",
    "    MONGO_EXTERNAL_MANAGER=os.getenv(\"MONGO_MANAGER_DATABASE_R_ONLY_URI\", \"mongodb://external:external_pass@localhost:27016/ParkMan_manager_db\")\n",
    "    external_client_managerDB=MongoClient(MONGO_EXTERNAL_MANAGER)\n",
    "    db_external_manager=external_client_managerDB.get_database()\n",
    "\n",
    "    ### Timescale DB connection\n",
    "    TIMESCALE_DB_URI=os.getenv(\"TIMESCALE_DB_DATABASE_URI\", \"postgresql://postgres_user:postgres_pass@localhost:5432/ParkingTransactionsDB\")\n",
    "    timescale_conn=connect(TIMESCALE_DB_URI)\n",
    "    timescale_cursor=timescale_conn.cursor(cursor_factory=RealDictCursor)\n",
    "\n",
    "\n",
    "\n",
    "    ###clickhouse target connect\n",
    "    CLICKHOUSE_PORT=os.getenv(\"CLICKHOUSE_PORT\", 8123)\n",
    "    CLICKHOUSE_HOST=os.getenv(\"CLICKHOUSE_HOST\", \"localhost\")\n",
    "    CLICKHOUSE_USER=os.getenv(\"CLICKHOUSE_USER\", \"parkman_user\")\n",
    "    CLICKHOUSE_PASS=os.getenv(\"CLICKHOUSE_PASS\", \"parkman_user_pass\")\n",
    "\n",
    "    client = clickhouse_connect.get_client(host=CLICKHOUSE_HOST, port=CLICKHOUSE_PORT, user=CLICKHOUSE_USER, password=CLICKHOUSE_PASS)\n",
    "\n",
    "\n",
    "\n",
    "    ##Expose metrics in server on port 9395\n",
    "    \n",
    "    start_http_server(9395)\n",
    "\n",
    "\n",
    "    ## define schedule (every 3 mins)\n",
    "    schedule.every(3).minutes.do(exec_etl)\n",
    "\n",
    "    #exec job in schedule\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        tm.sleep(1)\n",
    "\n",
    "\n",
    "    #print(f\"Imported data into clickhouse range {begin_range}, {end_range}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify python-etl.yml so that it exposes port 9395 for prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../python-etl.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../python-etl.yml\n",
    "version: '3'\n",
    "\n",
    "services:\n",
    "  etl:\n",
    "    build: ./ETL_BATCH\n",
    "    container_name: etl_container\n",
    "    expose:\n",
    "        - \"9395\"\n",
    "    env_file:\n",
    "        - ./ETL_BATCH/.env\n",
    "    depends_on:\n",
    "        mongo_manager_db_service:\n",
    "            condition: service_healthy\n",
    "        mongo_user_db_service:\n",
    "            condition: service_healthy\n",
    "        clickhouse:\n",
    "            condition: service_healthy\n",
    "\n",
    "    networks:\n",
    "        - app-network\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
